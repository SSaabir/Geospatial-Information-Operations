{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c56de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END, START\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_agent, load_tools\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langgraph'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import TypedDict\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from langchain.tools import tool\n",
    "import urllib.request\n",
    "import json\n",
    "import psycopg2\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.chains import create_sql_query_chain\n",
    "import re, json, time, os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb096f5d",
   "metadata": {},
   "source": [
    "# Notebook overview — imports and purpose\n",
    "\n",
    "This cell documents the imports and the high-level role of this notebook.\n",
    "\n",
    "- langgraph.graph.StateGraph, START, END: used to build a simple state-machine graph where nodes represent processing steps (used later to define the collector flow).\n",
    "- langchain.agents.initialize_agent, load_tools: load and wire external tools (APIs, wrappers) into a LangChain-style agent.\n",
    "- ChatOpenAI / ChatGroq / ChatGoogleGenerativeAI: LLM client wrappers used to instantiate a language model backend. The notebook uses `ChatGroq` for the main LLM.\n",
    "- TypedDict: used to define a typed state dictionary for LangGraph nodes.\n",
    "- numpy / pandas: data handling and preprocessing.\n",
    "- sqlalchemy.create_engine, psycopg2: database connection and low-level DB access.\n",
    "- langchain.tools.tool: decorator used to expose Python functions as agent tools.\n",
    "- langchain_experimental.sql.SQLDatabaseChain, langchain_community.utilities.SQLDatabase, create_sql_query_chain: utilities for converting NL -> SQL and executing queries safely.\n",
    "- urllib.request, json, requests (used later): HTTP calls / JSON parsing.\n",
    "- re, time, os, datetime: small utilities used across tools.\n",
    "\n",
    "Security note: several API keys and a DB connection string appear later in the notebook. Move secrets to environment variables (`os.environ`) or a secrets manager before production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64400ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "               groq_api_key=\"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\"\n",
    "               )\n",
    "\n",
    "# Define tools (APIs, DB connectors)\n",
    "tools = load_tools([\"serpapi\", \"requests_all\"], \n",
    "                   serpapi_api_key = \"49a312e94db629a1d7d4efa33647dc82322dd921680f5cbe1441de0aee587bbd\",\n",
    "                   allow_dangerous_tools=True\n",
    "                   )\n",
    "\n",
    "# 2️⃣ Create SQLAlchemy engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:ElDiabloX32@localhost:5432/GISDb')\n",
    "db = SQLDatabase(engine)\n",
    "sql_chain = create_sql_query_chain(llm, db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a890e",
   "metadata": {},
   "source": [
    "## LLM, tools and database setup\n",
    "\n",
    "This block configures the language model, external tools, and the database connection used by the agent:\n",
    "\n",
    "- `llm = ChatGroq(...)`: creates the LLM client used for prompting and SQL generation. Replace hard-coded API keys with environment variables before sharing or deploying.\n",
    "- `tools = load_tools([...])`: registers external helper tools (e.g., web search via SerpAPI, HTTP via `requests_all`) so the agent can call them.\n",
    "- `engine = create_engine(...)` and `db = SQLDatabase(engine)`: create an SQLAlchemy engine and wrap it with LangChain's `SQLDatabase` utility so higher-level chains can query the database.\n",
    "- `sql_chain = create_sql_query_chain(llm, db)`: wires the LLM and DB together so later code can convert natural language questions into SQL safely.\n",
    "\n",
    "Operational notes:\n",
    "- Secrets (API keys, DB passwords) must live in `os.environ` or a secrets manager, not inside notebook cells.\n",
    "- `SQLDatabase` is a wrapper — some code later expects `.run()` while other libraries expect a raw SQLAlchemy engine. Keep an eye for mismatches between `engine` vs `db` usage (see notes in the upload cell explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d814105",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8129ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "\n",
    "def extract_date_or_placeholder(sunrise_val):\n",
    "    if pd.isna(sunrise_val):\n",
    "        return \"###\"  \n",
    "    return sunrise_val.date()\n",
    "\n",
    "\n",
    "def preprocess_weather_data_csv(df):\n",
    "\n",
    "    df = df.drop(columns=['feelslike','feelslikemax','feelslikemin','dew','precipprob','precipcover','severerisk','stations','severerisk'])\n",
    "\n",
    "    df[\"sunrise\"] = pd.to_datetime(df[\"sunrise\"], errors=\"coerce\")\n",
    "    df[\"datetime\"] = df[\"sunrise\"].apply(extract_date_or_placeholder)\n",
    "    df[\"sunrise\"] = df[\"sunrise\"].dt.time\n",
    "    \n",
    "    if \"sunset\" in df.columns:\n",
    "        df[\"sunset\"] = pd.to_datetime(df[\"sunset\"], errors=\"coerce\").dt.time\n",
    "\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.replace(\"\", \"\")\n",
    "    df[\"conditions\"] = df[\"conditions\"].astype(str).str.replace(\",\", \"\")\n",
    "    df[\"country\"] = \"Sri Lanka\"\n",
    "    df.head()\n",
    "\n",
    "    df = df.rename(columns={\n",
    "    \"name\": \"statedistrict\",\n",
    "    \"precip\": \"rainsum\",\n",
    "    \"preciptype\": \"rain\",\n",
    "    \"tempmax\": \"tempmax\",\n",
    "    \"tempmin\": \"tempmin\",\n",
    "    \"temp\": \"temp\",\n",
    "    \"humidity\": \"humidity\",\n",
    "    \"snow\": \"snow\",\n",
    "    \"snowdepth\": \"snowdepth\",\n",
    "    \"windgust\": \"windgust\",\n",
    "    \"windspeed\": \"windspeed\",\n",
    "    \"winddir\": \"winddir\",\n",
    "    \"sealevelpressure\": \"sealevelpressure\",\n",
    "    \"cloudcover\": \"cloudcover\",\n",
    "    \"visibility\": \"visibility\",\n",
    "    \"solarradiation\": \"solarradiation\",\n",
    "    \"solarenergy\": \"solarenergy\",\n",
    "    \"uvindex\": \"uvindex\",\n",
    "    \"sunrise\": \"sunrise\",\n",
    "    \"sunset\": \"sunset\",\n",
    "    \"moonphase\": \"moonphase\",\n",
    "    \"conditions\": \"conditions\",\n",
    "    \"description\": \"description\",\n",
    "    \"icon\": \"icon\",\n",
    "    \"country\": \"country\"\n",
    "    })\n",
    "\n",
    "    for col in ['snow', 'rain']:\n",
    "        # Convert existing values to boolean: True if any value exists, False if NaN or empty\n",
    "        df[col] = df[col].apply(lambda x: True if pd.notna(x) and x != \"\" else False)\n",
    "\n",
    "\n",
    "    output_path = \"preprocessed_climate_dataset5.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"✅ Preprocessing completed. Saved to:\", output_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ba7c2",
   "metadata": {},
   "source": [
    "## Preprocessing function — what it does and why\n",
    "\n",
    "This block defines data cleaning and normalization before inserting weather records:\n",
    "\n",
    "- `extract_date_or_placeholder`: helper that returns a date or a placeholder if sunrise is missing.\n",
    "- `preprocess_weather_data_csv(df)`: main preprocessing routine. Steps:\n",
    "  1. Drops a set of unused or duplicate columns to reduce noise.\n",
    "  2. Parses `sunrise` (and optionally `sunset`) into datetime/time types and creates a `datetime` date column.\n",
    "  3. Cleans text columns (`name` → `statedistrict`, `conditions`) and sets `country` to a default.\n",
    "  4. Renames many incoming fields into your internal schema names.\n",
    "  5. Converts `snow` and `rain` into booleans.\n",
    "  6. Saves the cleaned dataset to `preprocessed_climate_dataset5.csv` and returns the DataFrame.\n",
    "\n",
    "Edge cases and suggestions:\n",
    "- If `sunrise` values are inconsistent, the `errors='coerce'` helps, but downstream code must handle `NaT`/placeholder values.\n",
    "- If the input file already matches your schema, consider making the rename optional to avoid accidental data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daafc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"upload_to_postgresql\", return_direct=True)\n",
    "def upload_to_postgresql(file_path: str) -> str:\n",
    "    \"\"\"Upload a CSV file into the PostgreSQL database.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = preprocess_weather_data_csv(df)\n",
    "    \n",
    "    # 1️⃣ Replace missing values\n",
    "    # Text columns → \"N/A\"\n",
    "    text_cols = ['statedistrict', 'conditions', 'description', 'icon', 'country']\n",
    "    df[text_cols] = df[text_cols].fillna(\"N/A\").replace(\"\", \"N/A\")\n",
    "\n",
    "    # Numeric columns → 0\n",
    "    num_cols = [\n",
    "        'tempmax', 'tempmin', 'temp', 'humidity', 'rainsum', 'snow', 'snowdepth',\n",
    "        'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover',\n",
    "        'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'moonphase'\n",
    "    ]\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "    # Boolean columns → False\n",
    "    bool_cols = ['rain', 'snow']\n",
    "    df[bool_cols] = df[bool_cols].fillna(False)\n",
    "\n",
    "\n",
    "    # Convert datetime/time columns\n",
    "    df['datetime'] = pd.to_datetime(df['datetime']).dt.date\n",
    "    df['sunrise'] = pd.to_datetime(df['sunrise'], format='%H:%M:%S').dt.time\n",
    "    df['sunset']  = pd.to_datetime(df['sunset'], format='%H:%M:%S').dt.time\n",
    "\n",
    "    # 3️⃣ Insert into PostgreSQL table\n",
    "    df.to_sql('weather_data', db, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c91c97",
   "metadata": {},
   "source": [
    "## `upload_to_postgresql` tool — CSV → PostgreSQL\n",
    "\n",
    "This `@tool`-decorated function is exposed to the agent to ingest a CSV and store weather rows:\n",
    "\n",
    "- Flow:\n",
    "  1. Reads CSV into pandas, calls the preprocessing routine.\n",
    "  2. Fills missing values: text columns → \"N/A\", numeric → 0, booleans → False.\n",
    "  3. Parses `datetime`, `sunrise`, and `sunset` into date/time objects.\n",
    "  4. Calls `df.to_sql('weather_data', db, if_exists='append', index=False)` to insert rows.\n",
    "\n",
    "Important implementation notes and potential bug:\n",
    "- `pandas.DataFrame.to_sql()` usually accepts a SQLAlchemy engine/connection, not a LangChain `SQLDatabase` wrapper. If `db` is a `SQLDatabase` object, that call may fail at runtime. Use `engine` or `engine.connect()` with `to_sql`, or convert the DataFrame rows to parameterized INSERTs.\n",
    "- Consider using parameterized inserts (psycopg2 or SQLAlchemy `execute_many`) for large files to improve performance and avoid SQL injection risks.\n",
    "- Retain an audit log or deduplication key to avoid inserting duplicate rows on retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cf9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"query_postgresql_tool\", return_direct=True)\n",
    "def query_postgresql_tool(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Safely convert a natural-language question into a SQL SELECT using LangChain's SQLDatabaseChain.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    audit_path = os.environ.get(\"SQL_AUDIT_LOG\", \"sql_audit.log\")\n",
    "\n",
    "    # --- helper to clean SQL ---\n",
    "    def extract_sql(text: str) -> str:\n",
    "        # Capture SQL inside ```sql ... ```\n",
    "        match = re.search(r\"```sql\\s+(.*?)\\s+```\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        # Otherwise, return the first SELECT/WITH onwards\n",
    "        match = re.search(r\"(select|with)\\b.*\", text, re.IGNORECASE | re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(0).strip()\n",
    "        return text.strip()\n",
    "\n",
    "    # Step 1: generate SQL\n",
    "    try:\n",
    "        sql_raw = sql_chain.invoke({\"question\": question})\n",
    "        sql_clean = extract_sql(str(sql_raw)).rstrip(\";\")\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"sql_generation_failed\", \"details\": str(e)})\n",
    "\n",
    "    # Step 2: safety checks\n",
    "    banned = r\"\\b(drop|delete|update|insert|alter|grant|truncate|create|replace|merge|shutdown)\\b\"\n",
    "    if re.search(banned, sql_clean, flags=re.IGNORECASE):\n",
    "        return json.dumps({\"error\": \"disallowed_statement\"})\n",
    "\n",
    "    if not re.match(r\"^\\s*(select|with)\\b\", sql_clean, flags=re.IGNORECASE):\n",
    "        return json.dumps({\"error\": \"not_select\", \"raw\": str(sql_raw)})\n",
    "\n",
    "    # Step 3: enforce LIMIT\n",
    "    if not re.search(r\"\\blimit\\b\", sql_clean, flags=re.IGNORECASE):\n",
    "        sql_exec = sql_clean + \" LIMIT 100\"\n",
    "    else:\n",
    "        sql_exec = sql_clean\n",
    "\n",
    "    # Step 4: audit\n",
    "    try:\n",
    "        with open(audit_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')}  QUESTION: {question}  SQL: {sql_exec}\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Step 5: execute query\n",
    "    try:\n",
    "        raw = db.run(sql_exec)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"execution_failed\", \"details\": str(e), \"sql\": sql_exec})\n",
    "\n",
    "    # Normalize rows\n",
    "    def normalize_rows(r):\n",
    "        if isinstance(r, list):\n",
    "            return [dict(row) if hasattr(row, \"keys\") else list(row) for row in r]\n",
    "        return str(r)\n",
    "\n",
    "    rows = normalize_rows(raw)\n",
    "    output = {\"sql\": sql_exec, \"row_count\": len(rows), \"rows\": rows}\n",
    "\n",
    "    return json.dumps(output, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0245d",
   "metadata": {},
   "source": [
    "## `query_postgresql_tool` — safe natural-language to SQL\n",
    "\n",
    "This tool converts a natural-language question into a SQL `SELECT` and executes it, with multiple safety layers:\n",
    "\n",
    "- Steps:\n",
    "  1. Uses `sql_chain.invoke({\"question\": question})` to ask the LLM to produce SQL.\n",
    "  2. `extract_sql()` pulls SQL from fenced ```sql blocks or finds the first `SELECT`/`WITH` segment.\n",
    "  3. Safety checks:\n",
    "     - Banned keywords (DROP, DELETE, UPDATE, INSERT, ALTER, etc.) block dangerous statements.\n",
    "     - Ensures the SQL starts with `SELECT`/`WITH`.\n",
    "     - Enforces a `LIMIT` of 100 if none provided.\n",
    "  4. Audits the query to a log file.\n",
    "  5. Executes the query via `db.run(sql_exec)` and normalizes the results into JSON.\n",
    "\n",
    "Security & operational notes:\n",
    "- This function intentionally prevents data-modifying SQL. Do not remove the banned pattern checks unless you add strict RBAC and auditing.\n",
    "- If you expect large result sets, either paginate or increase the enforced limit intentionally.\n",
    "- The function returns structured JSON with `sql`, `row_count`, and `rows` to make it easy for agents to consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool(\"fetch_weather_tool\", return_direct=True)\n",
    "def fetch_weather_tool (tool_input: str) -> str:\n",
    "    \"\"\"\n",
    "    tool_input: expected format \"city=Colombo;date=yesterday\"\n",
    "    \"\"\"\n",
    "    # Parse input\n",
    "    params = dict(item.split(\"=\") for item in tool_input.split(\";\"))\n",
    "    city = params.get(\"city\", \"Colombo\")\n",
    "    date = params.get(\"date\", \"yesterday\")\n",
    "\n",
    "    # Call the original function\n",
    "    weather_info = fetch_and_store_weather(city, date)\n",
    "    print(weather_info)\n",
    "    # Return as JSON string for the agent\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "def fetch_and_store_weather(city=\"Colombo\", date=\"yesterday\"):\n",
    "    \"\"\"Fetch weather data from API and store in PostgreSQL.\"\"\"\n",
    "    # API Call\n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{city}/{date}?unitGroup=metric&include=days&key=KGCW7SXGVXRYL7ZK7W7SEJSR8&contentType=json\"\n",
    "    ResultBytes = urllib.request.urlopen(url)\n",
    "    jsonData = json.load(ResultBytes)\n",
    "    day = jsonData[\"days\"][0]\n",
    "\n",
    "    # Map to schema\n",
    "    weather_info = {\n",
    "        \"country\": \"Sri Lanka\",\n",
    "        \"statedistrict\": city,\n",
    "        \"datetime\": day[\"datetime\"],\n",
    "        \"tempmax\": day.get(\"tempmax\"),\n",
    "        \"tempmin\": day.get(\"tempmin\"),\n",
    "        \"temp\": day.get(\"temp\"),\n",
    "        \"humidity\": day.get(\"humidity\"),\n",
    "        \"rain\": day.get(\"precip\", 0) > 0,\n",
    "        \"rainsum\": day.get(\"precip\"),\n",
    "        \"snow\": day.get(\"snow\", 0) > 0,\n",
    "        \"snowdepth\": day.get(\"snowdepth\"),\n",
    "        \"windgust\": day.get(\"windgust\"),\n",
    "        \"windspeed\": day.get(\"windspeed\"),\n",
    "        \"winddir\": day.get(\"winddir\"),\n",
    "        \"sealevelpressure\": day.get(\"pressure\"),\n",
    "        \"cloudcover\": day.get(\"cloudcover\"),\n",
    "        \"visibility\": day.get(\"visibility\"),\n",
    "        \"solarradiation\": day.get(\"solarradiation\"),\n",
    "        \"solarenergy\": day.get(\"solarenergy\"),\n",
    "        \"uvindex\": day.get(\"uvindex\"),\n",
    "        \"sunrise\": day.get(\"sunrise\"),\n",
    "        \"sunset\": day.get(\"sunset\"),\n",
    "        \"moonphase\": day.get(\"moonphase\"),\n",
    "        \"conditions\": day.get(\"conditions\"),\n",
    "        \"description\": day.get(\"description\"),\n",
    "        \"icon\": day.get(\"icon\")\n",
    "    }\n",
    "\n",
    "    # Insert into DB\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"GISDb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"ElDiabloX32\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO weather_data (\n",
    "        country, statedistrict, datetime, tempmax, tempmin, temp, humidity,\n",
    "        rain, rainsum, snow, snowdepth, windgust, windspeed, winddir,\n",
    "        sealevelpressure, cloudcover, visibility, solarradiation, solarenergy,\n",
    "        uvindex, sunrise, sunset, moonphase, conditions, description, icon\n",
    "    ) VALUES (\n",
    "        %(country)s, %(statedistrict)s, %(datetime)s, %(tempmax)s, %(tempmin)s, %(temp)s, %(humidity)s,\n",
    "        %(rain)s, %(rainsum)s, %(snow)s, %(snowdepth)s, %(windgust)s, %(windspeed)s, %(winddir)s,\n",
    "        %(sealevelpressure)s, %(cloudcover)s, %(visibility)s, %(solarradiation)s, %(solarenergy)s,\n",
    "        %(uvindex)s, %(sunrise)s, %(sunset)s, %(moonphase)s, %(conditions)s, %(description)s, %(icon)s\n",
    "    )\n",
    "    \"\"\", weather_info)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return weather_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786183a4",
   "metadata": {},
   "source": [
    "## Weather fetcher — `fetch_weather_tool` and `fetch_and_store_weather`\n",
    "\n",
    "This area implements fetching weather from VisualCrossing and storing it directly in Postgres:\n",
    "\n",
    "- `fetch_weather_tool`: a thin wrapper that parses `tool_input` (format `city=Colombo;date=yesterday`), calls `fetch_and_store_weather`, prints, and returns JSON.\n",
    "- `fetch_and_store_weather`:\n",
    "  - Calls VisualCrossing's timeline API with a provided API key.\n",
    "  - Extracts the day's data (`jsonData['days'][0]`) and maps fields into the internal schema (`weather_info`).\n",
    "  - Opens a `psycopg2` connection and executes a parameterized `INSERT` into `weather_data` using a mapping dict (`%(country)s`, etc.), then commits.\n",
    "\n",
    "Notes and improvements:\n",
    "- The insertion uses parameterized placeholders (good). Keep credentials out of the notebook.\n",
    "- The VisualCrossing response shape can vary; consider wrapping extraction in safety checks to avoid KeyError on missing `days`.\n",
    "- Add retry/backoff for HTTP errors, and handle API rate limits.\n",
    "- Consider using a connection pool (e.g., `psycopg2.pool`) instead of opening/closing connections for each call if this runs frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812bb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n@tool(\"fetch_climate_news\", return_direct=True)\\ndef fetch_climate_news(query: str = \"climate change\"):\\n\\n\\n\\n    import requests\\n\\n    api_key = \"f875db6eac964594bbcd54e77f9d9b22\"\\n    url = \"https://newsapi.org/v2/everything\"\\n    params = {\\n        \"q\": query,\\n        \"language\": \"en\",\\n        \"sortBy\": \"publishedAt\",\\n        \"pageSize\": 5,\\n        \"apiKey\": api_key,\\n    }\\n\\n    try:\\n        resp = requests.get(url, params=params, timeout=10)\\n        data = resp.json()\\n    except Exception:\\n        return {\\n            \"title\": [None]*5,\\n            \"url\": [None]*5,\\n            \"publishedAt\": [None]*5\\n        }\\n\\n    articles = data.get(\"articles\", [])\\n    titles, urls, dates = [], [], []\\n    for a in articles[:5]:\\n        titles.append(a.get(\"title\"))\\n        urls.append(a.get(\"url\"))\\n        dates.append(a.get(\"publishedAt\"))\\n\\n    # Pad lists if fewer than 5 articles\\n    while len(titles) < 5:\\n        titles.append(None)\\n        urls.append(None)\\n        dates.append(None)\\n\\n    return {\\n        \"title\": titles,\\n        \"url\": urls,\\n        \"publishedAt\": dates\\n    }\\n    \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "@tool(\"fetch_climate_news\", return_direct=True)\n",
    "def fetch_climate_news(query: str = \"climate change\"):\n",
    "\n",
    "\n",
    "\n",
    "    import requests\n",
    "\n",
    "    api_key = \"f875db6eac964594bbcd54e77f9d9b22\"\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"language\": \"en\",\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"pageSize\": 5,\n",
    "        \"apiKey\": api_key,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=10)\n",
    "        data = resp.json()\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"title\": [None]*5,\n",
    "            \"url\": [None]*5,\n",
    "            \"publishedAt\": [None]*5\n",
    "        }\n",
    "\n",
    "    articles = data.get(\"articles\", [])\n",
    "    titles, urls, dates = [], [], []\n",
    "    for a in articles[:5]:\n",
    "        titles.append(a.get(\"title\"))\n",
    "        urls.append(a.get(\"url\"))\n",
    "        dates.append(a.get(\"publishedAt\"))\n",
    "\n",
    "    # Pad lists if fewer than 5 articles\n",
    "    while len(titles) < 5:\n",
    "        titles.append(None)\n",
    "        urls.append(None)\n",
    "        dates.append(None)\n",
    "\n",
    "    return {\n",
    "        \"title\": titles,\n",
    "        \"url\": urls,\n",
    "        \"publishedAt\": dates\n",
    "    }\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ade9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"fetch_extra_earth_data\", return_direct=True)\n",
    "def fetch_extra_earth_data(location: str = \"colombo\") -> str:\n",
    "    \"\"\"Fetch air quality metrics (pm10, pm2_5, carbon_monoxide, ozone) using Open-Meteo.\n",
    "\n",
    "    This uses Open-Meteo's Air Quality API (no API key). `location` can be a city name or \"lat,lon\".\n",
    "    Returns a JSON string with the latest available values or an error dict.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    def geocode_city(city_name: str):\n",
    "        # Simple geocode via Nominatim (OpenStreetMap) - no key but rate-limited\n",
    "        try:\n",
    "            r = requests.get(\"https://nominatim.openstreetmap.org/search\", params={\"q\": city_name, \"format\": \"json\", \"limit\": 1}, headers={\"User-Agent\": \"collector-agent/1.0\"}, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                j = r.json()\n",
    "                if j:\n",
    "                    return float(j[0][\"lat\"]), float(j[0][\"lon\"])\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    # Parse location: allow \"lat,lon\" or city name\n",
    "    lat_lon_match = re.match(r\"^\\s*([-+]?\\d+\\.?\\d*)\\s*,\\s*([-+]?\\d+\\.?\\d*)\\s*$\", location)\n",
    "    if lat_lon_match:\n",
    "        lat, lon = float(lat_lon_match.group(1)), float(lat_lon_match.group(2))\n",
    "    else:\n",
    "        gc = geocode_city(location)\n",
    "        if gc is None:\n",
    "            # fallback to Colombo coordinates\n",
    "            lat, lon = 6.9271, 79.8612\n",
    "        else:\n",
    "            lat, lon = gc\n",
    "\n",
    "    # Open-Meteo Air Quality API\n",
    "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "    \"latitude\": lat,\n",
    "    \"longitude\": lon,\n",
    "    \"hourly\": \"pm10,pm2_5,carbon_monoxide,ozone\"\n",
    "}\n",
    "    try:\n",
    "        resp = requests.get(url, params=params, timeout=10)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"request_failed\", \"details\": str(e)})\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        return json.dumps({\"error\": \"open_meteo_error\", \"status_code\": resp.status_code, \"details\": resp.text})\n",
    "\n",
    "    try:\n",
    "        data = resp.json()\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"invalid_json\", \"details\": str(e)})\n",
    "\n",
    "    # Get latest index (last hourly point)\n",
    "    hourly = data.get(\"hourly\", {})\n",
    "    times = hourly.get(\"time\", [])\n",
    "    if not times:\n",
    "        return json.dumps({\"error\": \"no_hourly_data\"})\n",
    "\n",
    "    idx = -1\n",
    "    try:\n",
    "        latest = {\n",
    "            \"time\": times[idx],\n",
    "            \"pm10\": hourly.get(\"pm10\", [None])[idx],\n",
    "            \"pm2_5\": hourly.get(\"pm2_5\", [None])[idx],\n",
    "            \"carbon_monoxide\": hourly.get(\"carbon_monoxide\", [None])[idx],\n",
    "            \"ozone\": hourly.get(\"ozone\", [None])[idx],\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"parse_error\", \"details\": str(e), \"raw\": data})\n",
    "\n",
    "    return json.dumps(latest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e40975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"upload_air_quality_to_postgres\", return_direct=True)\n",
    "def upload_air_quality_to_postgres(location: str = \"Colombo\") -> str:\n",
    "    \"\"\"\n",
    "    Fetch latest air quality data via fetch_extra_earth_data and upload to PostgreSQL.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        # Use the existing tool instead of refetching\n",
    "        data_json = fetch_extra_earth_data.invoke(location)\n",
    "        data = json.loads(data_json)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"fetch_failed\", \"details\": str(e)})\n",
    "\n",
    "    # Check for errors in fetch\n",
    "    if data.get(\"error\"):\n",
    "        return json.dumps({\"error\": \"fetch_error\", \"details\": data})\n",
    "\n",
    "    # Extract fields, safely using None if missing\n",
    "    datetime_val = data.get(\"time\") or str(datetime.today().date())\n",
    "    pm10 = data.get(\"pm10\")\n",
    "    pm2_5 = data.get(\"pm2_5\")\n",
    "    carbon_monoxide = data.get(\"carbon_monoxide\")\n",
    "    ozone = data.get(\"ozone\")\n",
    "    lat = data.get(\"lat\")\n",
    "    lon = data.get(\"lon\")\n",
    "    country = data.get(\"country\") or \"Sri Lanka\"  # optional default\n",
    "    statedistrict = location\n",
    "    source = \"Open-Meteo Air Quality API\"\n",
    "\n",
    "    # Insert into PostgreSQL\n",
    "    try:\n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO air_quality_data (\n",
    "            country, statedistrict, datetime, pm10, pm2_5, carbon_monoxide, ozone, lat, lon, source\n",
    "        ) VALUES (\n",
    "            '{country}', '{statedistrict}', '{datetime_val}', {pm10}, {pm2_5}, {carbon_monoxide}, {ozone}, {lat}, {lon}, '{source}'\n",
    "        );\n",
    "        \"\"\"\n",
    "        db.run(sql)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": \"db_insert_failed\", \"details\": str(e), \"sql\": sql})\n",
    "\n",
    "    return json.dumps({\"success\": True, \"uploaded_data\": data}, default=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de025cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11796\\3521791818.py:16: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  collector = initialize_agent(\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_11796\\3521791818.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = collector.run(state[\"input\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer this question, I need to understand what the `upload_air_quality_to_postgres` function does and how it interacts with the `fetch_extra_earth_data` function and the PostgreSQL database.\n",
      "\n",
      "Thought: The `upload_air_quality_to_postgres` function seems to fetch the latest air quality data for a given location and then upload it to a PostgreSQL database. I need to call this function with the location 'Colombo' as the argument.\n",
      "\n",
      "Action: upload_air_quality_to_postgres\n",
      "Action Input: location='Colombo'\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{\"error\": \"db_insert_failed\", \"details\": \"(psycopg2.errors.SyntaxError) syntax error at or near \\\"Colombo\\\"\\nLINE 5:             'Sri Lanka', 'location='Colombo'', '2025-09-27T2...\\n                                            ^\\n\\n[SQL: \\n        INSERT INTO air_quality_data (\\n            country, statedistrict, datetime, pm10, pm2_5, carbon_monoxide, ozone, lat, lon, source\\n        ) VALUES (\\n            'Sri Lanka', 'location='Colombo'', '2025-09-27T23:00', None, None, None, None, 6.8994685, 79.8691523, 'Open-Meteo Air Quality API'\\n        );\\n        ]\\n(Background on this error at: https://sqlalche.me/e/20/f405)\", \"sql\": \"\\n        INSERT INTO air_quality_data (\\n            country, statedistrict, datetime, pm10, pm2_5, carbon_monoxide, ozone, lat, lon, source\\n        ) VALUES (\\n            'Sri Lanka', 'location='Colombo'', '2025-09-27T23:00', None, None, None, None, 6.8994685, 79.8691523, 'Open-Meteo Air Quality API'\\n        );\\n        \"}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"error\": \"db_insert_failed\", \"details\": \"(psycopg2.errors.SyntaxError) syntax error at or near \\\"Colombo\\\"\\nLINE 5:             'Sri Lanka', 'location='Colombo'', '2025-09-27T2...\\n                                            ^\\n\\n[SQL: \\n        INSERT INTO air_quality_data (\\n            country, statedistrict, datetime, pm10, pm2_5, carbon_monoxide, ozone, lat, lon, source\\n        ) VALUES (\\n            'Sri Lanka', 'location='Colombo'', '2025-09-27T23:00', None, None, None, None, 6.8994685, 79.8691523, 'Open-Meteo Air Quality API'\\n        );\\n        ]\\n(Background on this error at: https://sqlalche.me/e/20/f405)\", \"sql\": \"\\n        INSERT INTO air_quality_data (\\n            country, statedistrict, datetime, pm10, pm2_5, carbon_monoxide, ozone, lat, lon, source\\n        ) VALUES (\\n            'Sri Lanka', 'location='Colombo'', '2025-09-27T23:00', None, None, None, None, 6.8994685, 79.8691523, 'Open-Meteo Air Quality API'\\n        );\\n        \"}\n"
     ]
    }
   ],
   "source": [
    "tools.append(upload_to_postgresql)\n",
    "tools.append(fetch_weather_tool)\n",
    "# add the new tools we inserted above\n",
    "# fetch_climate_news and fetch_extra_earth_data are defined in earlier cells\n",
    "try:\n",
    "    #tools.append(fetch_climate_news)\n",
    "    tools.append(fetch_extra_earth_data)\n",
    "    tools.append(query_postgresql_tool)\n",
    "    tools.append(upload_air_quality_to_postgres)\n",
    "except NameError:\n",
    "    # In case the notebook is executed top-to-bottom and the cells haven't been run yet,\n",
    "    # we proceed silently; the agent will fail to initialize until those cells are run.\n",
    "    pass\n",
    "\n",
    "# Collector agent\n",
    "collector = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", verbose=True,\n",
    ")\n",
    "\n",
    "class collectorState(TypedDict):\n",
    "    input: str  # input query\n",
    "    title: list | None\n",
    "    url: list | None\n",
    "    publishedAt: list | None\n",
    "    output: str  # collected data\n",
    "    \n",
    "# Define LangGraph nodes\n",
    "graph = StateGraph(collectorState)\n",
    "\n",
    "def collector_node(state: collectorState) -> collectorState:\n",
    "    result = collector.run(state[\"input\"])\n",
    "    \n",
    "    if isinstance(result, dict):\n",
    "        state[\"title\"] = result.get(\"title\")\n",
    "        state[\"url\"] = result.get(\"url\")\n",
    "        state[\"publishedAt\"] = result.get(\"publishedAt\")\n",
    "    else:\n",
    "        state[\"output\"] = result\n",
    "    \n",
    "    return state\n",
    "\n",
    "graph.add_node(\"collector\", collector_node)\n",
    "graph.add_edge(START, \"collector\")  # input query\n",
    "graph.add_edge(\"collector\", END)  # outputs collected data\n",
    "\n",
    "# Run graph\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"input\": \"upload_air_quality_to_postgres for Colombo\"})\n",
    "print(result[\"output\"])\n",
    "\n",
    "\n",
    "def run_collector_agent(query: str) -> str:\n",
    "    \"\"\"Run the collector agent with the given query and return the output.\"\"\"\n",
    "    result = app.invoke({\"input\": query})\n",
    "    return result[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93085b5",
   "metadata": {},
   "source": [
    "## Agent-to-Agent (A2A) connection — where and how to wire it\n",
    "\n",
    "Placement (exact spot)\n",
    "- Insert the A2A wiring immediately after the agent is created — i.e., right after the line that reads `collector = initialize_agent(...)` and before the `class collectorState(TypedDict):` / graph creation lines.\n",
    "- In this notebook the correct logical place is the same code cell that creates `collector` (or as the very next cell). This guarantees the collector agent and its `tools` list are available in the same scope when you expose or register another agent.\n",
    "\n",
    "Why here\n",
    "- The agent object and the `tools` list are already defined and loaded with helper functions. Registering the other agent (or a wrapper that calls it) at this point lets the collector call the other agent as a tool during execution.\n",
    "\n",
    "Two common A2A patterns\n",
    "1) In-process wrapper (recommended if both agents run in the same notebook/process)\n",
    "- Create a small `@tool` wrapper that calls the other agent's `.run()` method or function interface.\n",
    "- Append that wrapper to the `tools` list before the collector is used. Example contract:\n",
    "  - Input: JSON string or short instruction\n",
    "  - Output: JSON-serializable dict or plain text\n",
    "  - Error mode: raise or return an error object\n",
    "- Pros: low latency, simple. Cons: both agents share memory and resources (watch for blocking).\n",
    "\n",
    "2) Out-of-process RPC (HTTP/message queue)\n",
    "- If the other agent runs on another host or process, expose it via a small HTTP endpoint or a message queue (Redis, RabbitMQ). Register a `requests`-based tool that calls that endpoint.\n",
    "- Pros: isolation, better scaling and fault isolation. Cons: more infra and auth.\n",
    "\n",
    "Suggested minimal in-process wiring (where to insert)\n",
    "- Right after `collector = initialize_agent(...)` add:\n",
    "  - A `@tool(\"other_agent_tool\", return_direct=True)` wrapper that calls `other_agent.run(input)` (or the function entrypoint for Trend/Prediction agents) and returns JSON/text.\n",
    "  - `tools.append(other_agent_tool)` so the collector can call it by name.\n",
    "\n",
    "Contract to document and enforce\n",
    "- Input shape: string or JSON (example: `{ \"task\": \"analyze\", \"payload\": {...} }`).\n",
    "- Output shape: JSON with at least `{ \"status\": \"ok\"|\"error\", \"data\": ... }`.\n",
    "- Timeouts: enforce a call timeout (e.g., 10s) and return a clear error object on timeout.\n",
    "- Retries: 0-2 retries for transient network errors.\n",
    "- Authentication: if using RPC, add an HMAC or bearer token header; do not put tokens in notebook cells.\n",
    "\n",
    "Operational considerations\n",
    "- Concurrency: if the collector runs concurrently, ensure the called agent is thread-safe or use a queue to serialize work.\n",
    "- Rate limiting: enforce per-agent rate limits to avoid cascading overloads.\n",
    "- Observability: log every A2A call (caller, callee, latency, status) and surface errors.\n",
    "- Security: never allow anonymous arbitrary code execution via A2A. Limit the callee's abilities and validate inputs.\n",
    "- Fail-open vs fail-closed: prefer fail-closed for destructive operations and fail-open for best-effort augmentations.\n",
    "\n",
    "Example integration checklist (to perform at the placement point)\n",
    "- [ ] Implement `other_agent` or ensure its entrypoint is importable in this notebook.\n",
    "- [ ] Add `@tool` wrapper that calls `other_agent.run()` or performs HTTP/RPC with auth.\n",
    "- [ ] Append wrapper to `tools` before agent initialization is finalized (or right after, then re-initialize agent if necessary).\n",
    "- [ ] Add timeouts, retries, and error normalization in the wrapper.\n",
    "- [ ] Add logging/audit lines for A2A calls.\n",
    "\n",
    "Notes about re-initialization\n",
    "- Some agent frameworks snapshot tools at initialization. If you append a tool after initializing `collector`, either (A) append before initializing, or (B) recreate the agent (call `initialize_agent(...)` again) so the new tool is visible. The safer pattern is to append all tools (including other-agent wrappers) before calling `initialize_agent(...)`.\n",
    "\n",
    "If you want, I can insert a small example `@tool` wrapper (in-process) and show exactly which notebook cell to edit and the exact code to add. Which A2A pattern do you prefer: in-process wrapper or HTTP/RPC?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
